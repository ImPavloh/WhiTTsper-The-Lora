{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WhiTTsper the Lora\n",
        "(Whisper + TTS + Alpaca-Lora) - *I'm not good at naming things*\n",
        "\n",
        "*@ImPavloh*\n",
        "\n",
        "**Remember to switch the runtime environment to GPU before running the code.**"
      ],
      "metadata": {
        "id": "la-winTcNxk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Libraries and Dependencies**\n",
        "\n",
        "*Estimated installation time: around 2 minutes.*"
      ],
      "metadata": {
        "id": "KDmfHItfEJhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v8Pv_QmIByp"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q langdetect\n",
        "!pip install -q gradio\n",
        "!pip install -q torch\n",
        "!pip install -q gtts\n",
        "import torch\n",
        "import whisper\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "from peft import PeftModel\n",
        "from langdetect import detect\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and configure the Alpaca-lora (LLaMA) model with 7B parameters.**\n",
        "\n",
        "*Estimated execution time: around 3 minutes.*"
      ],
      "metadata": {
        "id": "Ta6Xc9osERga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\")"
      ],
      "metadata": {
        "id": "TuFUR-QpqHjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions that enable the construction of a task-based question and answer system."
      ],
      "metadata": {
        "id": "nrQM7lAQRE5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_instruction(instruction, input=None):\n",
        "    input_section = f\"### Input:\\n{input}\\n\" if input else \"\"\n",
        "    return f\"\"\"Here is an instruction that describes a task. Write a response that adequately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "{input_section}### Response:\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    max_length=512,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None, temperature=0.1):\n",
        "    prompt = gen_instruction(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    generation_output = modelo.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=GenerationConfig(\n",
        "            temperature=temperature,\n",
        "            top_p=0.75,\n",
        "            num_beams=4,\n",
        "            num_return_sequences=1,\n",
        "            max_length=512,\n",
        "        ),\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    sequence = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(sequence)\n",
        "    response_text = output.split(\"### Response:\")[-1].strip()\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "f2viuCuqqtMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio Interface**\n",
        "\n",
        "When you click on the recording button, the microphone will start recording. The user also has the option to select the size of the Whisper model.\n",
        "\n",
        "The larger the Whisper model, the better it will be, but it will take longer.\n",
        "\n",
        "1.   **Tiny** 39M parameters\n",
        "2.   **Base** 74M parameters **[recommended]**\n",
        "3.   **Small** 244M parameters\n",
        "4.   **Medium** 269M parameters\n",
        "5.   **Large** 1550M parameters\n",
        "\n",
        "\n",
        "Once the recording is submitted, the audio will be automatically transcribed with Whisper and LLaMa will respond with an audio. The message will appear in the conversation history along with the rest of the messages."
      ],
      "metadata": {
        "id": "JyASUjp3LTz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP\n",
        "\n",
        "def stt(tmp_filename, model_size, reset_conversation, temperature):\n",
        "    if reset_conversation:\n",
        "        gr.State.conversation_history = []\n",
        "        return \"\", None, None\n",
        "\n",
        "    if not tmp_filename or not model_size:\n",
        "        return \"Please record an audio, select a Whisper model or attach an audio file.\", None, None\n",
        "\n",
        "    if not hasattr(gr.State, \"conversation_history\"):\n",
        "        gr.State.conversation_history = []\n",
        "\n",
        "    conversation_history = gr.State.conversation_history\n",
        "\n",
        "    try:\n",
        "        whisper_model = whisper.load_model(model_size)\n",
        "        result = modelo_whisper.transcribe(tmp_filename)\n",
        "\n",
        "        text_input = result['text']\n",
        "        conversation_history.append(\"User: \" + text_input)\n",
        "        conversation_input = \"\\n\".join(conversation_history)\n",
        "        \n",
        "        response_text = evaluate(conversation_input, temperature=temperature)\n",
        "        \n",
        "        conversation_history.append(\"AI: \" + response_text.strip())\n",
        "\n",
        "        formatted_conversation_history = \"\\n\".join(conversation_history)\n",
        "\n",
        "        language = detect(response_text)\n",
        "        tts = gTTS(response_text, lang=language)\n",
        "        _, response_audio_path = tempfile.mkstemp(suffix='.mp3')\n",
        "        tts.save(response_audio_path)\n",
        "\n",
        "        return formatted_conversation_history, response_audio_path, response_text.strip()\n",
        "\n",
        "    except (ValueError, Exception) as e:\n",
        "        error_message = str(e)\n",
        "        return f\"Error: {error_message}\", None, None\n",
        "\n",
        "inputs = [\n",
        "    gr.Audio(source='microphone', type='filepath', label=\"Talk here üó£Ô∏è\"),\n",
        "    gr.Dropdown(choices=['tiny', 'base', 'small', 'medium', 'large'], value='base', label=\"Model size üì¶\"),\n",
        "    gr.Checkbox(label=\"Restart conversation\", value=False),\n",
        "    gr.Slider(min_value=0.1, max_value=1.0, step=0.1, value=0.1, label=\"Generation temperature üå°Ô∏è\"),\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Conversation history üìÑ\"),\n",
        "    gr.Audio(type='filepath', label=\"Response üîä\"),\n",
        "]\n",
        "\n",
        "gr.Interface(\n",
        "    fn=stt,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    allow_flagging='never',\n",
        "    title=\"üó£Ô∏è WhiTTsper the Lora ü¶ú\",\n",
        "    description=\"This demo uses speech recognition and speech synthesis technologies, OpenAI's Whisper and Google Text-to-Speech respectively, to interact with the Alpaca-LoRA AI ~ LLaMa 7B model.\",\n",
        "    css=\"footer {visibility: hidden}\"\n",
        "    ).launch()"
      ],
      "metadata": {
        "id": "z3CjDuXsqPWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
