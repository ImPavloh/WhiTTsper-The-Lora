{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **WhiTTsper the Lora**\n",
        "\n",
        "(Whisper + TTs + Alpaca-Lora) - *No soy bueno con los nombres*\n",
        "\n",
        "*@ImPavloh*\n",
        "\n",
        "**Recuerda cambiar el tipo de entorno de ejeccui√≥n a GPU antes de ejecutar el c√≥digo.**"
      ],
      "metadata": {
        "id": "la-winTcNxk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalar librer√≠as y dependencias**\n",
        " \n",
        "*~2 minutos de instalaci√≥n*"
      ],
      "metadata": {
        "id": "KDmfHItfEJhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v8Pv_QmIByp"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q langdetect\n",
        "!pip install -q gradio\n",
        "!pip install -q torch\n",
        "!pip install -q gtts\n",
        "import torch\n",
        "import whisper\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "from peft import PeftModel\n",
        "from langdetect import detect\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar y configurar el modelo Alpaca-lora (LLaMA) de 7B par√°metros.\n",
        "\n",
        "*~3 minutos de ejecuci√≥n*"
      ],
      "metadata": {
        "id": "Ta6Xc9osERga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "modelo = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "modelo = PeftModel.from_pretrained(modelo, \"tloen/alpaca-lora-7b\")"
      ],
      "metadata": {
        "id": "TuFUR-QpqHjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir *funciones* que permiten construir un **sistema de preguntas y respuestas basado en tareas**."
      ],
      "metadata": {
        "id": "nrQM7lAQRE5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_instruccion(instruccion, input=None):\n",
        "    input_section = f\"### Entrada:\\n{input}\\n\" if input else \"\"\n",
        "    return f\"\"\"A continuaci√≥n se muestra una instrucci√≥n que describe una tarea. Escribe una respuesta que complete adecuadamente la petici√≥n.\n",
        "\n",
        "### Instrucci√≥n:\n",
        "{instruccion}\n",
        "\n",
        "{input_section}### Respuesta:\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    max_length=512,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = gen_instruccion(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(modelo.device)\n",
        "    generation_output = modelo.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    sequence = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(sequence)\n",
        "    response_text = output.split(\"### Respuesta:\")[-1].strip()\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "f2viuCuqqtMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interfaz Gradio**\n",
        "\n",
        "Al hacer clic en el bot√≥n de grabaci√≥n, el micr√≥fono comenzar√° a grabar. El usuario tiene tambi√©n la opci√≥n de seleccionar el tama√±o del modelo de Whisper. \n",
        "\n",
        "Cuanto m√°s grande sea el modelo de Whisper mejor ser√° pero tardar√° m√°s.\n",
        "\n",
        "1.   **Tiny** 39 M par√°metros\n",
        "2.   **Base** 74 M par√°metros **[recomendado]**\n",
        "3.   **Small** 244 M par√°metros\n",
        "4.   **Medium** 269 M par√°metros\n",
        "5.   **Large** 1550 M par√°metros\n",
        "\n",
        "\n",
        "\n",
        "Una vez que se env√≠a la grabaci√≥n, se transcribir√° autom√°ticamente el audio con Whisper y LLaMa responder√° con un audio. El mensaje aparecer√° en el historial de conversaci√≥n junto al resto de mensajes."
      ],
      "metadata": {
        "id": "JyASUjp3LTz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP - Puede que haya alg√∫n fallo\n",
        "\n",
        "def stt(tmp_filename, model_size, reset_conversation):\n",
        "    if reset_conversation:\n",
        "        gr.State.conversation_history = []\n",
        "        return \"\", None, None\n",
        "\n",
        "    if not tmp_filename or not model_size:\n",
        "        return \"Por favor, graba un audio y selecciona un modelo de Whisper.\", None, None\n",
        "\n",
        "    if not hasattr(gr.State, \"conversation_history\"):\n",
        "        gr.State.conversation_history = []\n",
        "\n",
        "    conversation_history = gr.State.conversation_history\n",
        "\n",
        "    try:\n",
        "        modelo_whisper = whisper.load_model(model_size)\n",
        "        result = modelo_whisper.transcribe(tmp_filename)\n",
        "\n",
        "        text_input = result['text']\n",
        "        conversation_history.append(\"Usuario: \" + text_input)\n",
        "        conversation_input = \"\\n\".join(conversation_history)\n",
        "        \n",
        "        response_text = evaluate(conversation_input)\n",
        "        \n",
        "        conversation_history.append(\"IA: \" + response_text.strip())\n",
        "\n",
        "        formatted_conversation_history = \"\\n\".join(conversation_history)\n",
        "\n",
        "        idioma = detect(response_text)\n",
        "        tts = gTTS(response_text, lang=idioma)\n",
        "        _, response_audio_path = tempfile.mkstemp(suffix='.mp3')\n",
        "        tts.save(response_audio_path)\n",
        "\n",
        "        return formatted_conversation_history, response_audio_path, response_text.strip()\n",
        "\n",
        "    except (ValueError, Exception) as e:\n",
        "        error_message = str(e)\n",
        "        return f\"Error: {error_message}\", None, None\n",
        "\n",
        "inputs = [\n",
        "    gr.Audio(source='microphone', type='filepath', label=\"Habla aqu√≠ üó£Ô∏è\"),\n",
        "    gr.Dropdown(choices=['tiny', 'base', 'small', 'medium', 'large'], value='base', label=\"Tama√±o del modelo üì¶\"),\n",
        "    gr.Checkbox(label=\"Reiniciar conversaci√≥n\", value=False),\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Historial de conversaci√≥n üìÑ\"),\n",
        "    gr.Audio(type='filepath', label=\"Respuesta üîä\"),\n",
        "]\n",
        "\n",
        "gr.Interface(\n",
        "    fn=stt,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    allow_flagging='never',\n",
        "    title=\"üó£Ô∏è WhiTTsper the Lora ü¶ú\",\n",
        "    description=\"Esta demo utiliza tecnolog√≠as de reconocimiento de voz y s√≠ntesis de voz, Whisper de OpenAI y Google Text-to-Speech respectivamente, para interactuar con la IA Alpaca-LoRA ~ modelo LLaMa 7B ).\",\n",
        "    css=\"footer {visibility: hidden}\"\n",
        "    ).launch(debug=True)"
      ],
      "metadata": {
        "id": "z3CjDuXsqPWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
