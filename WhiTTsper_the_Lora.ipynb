{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImPavloh/WhiTTsper-The-Lora/blob/main/WhiTTsper_the_Lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WhiTTsper the Lora**\n",
        "\n",
        "(Whisper + TTs + Alpaca-Lora) - *No soy bueno con los nombres*\n",
        "\n",
        "**Recuerda cambiar el tipo de entorno de ejeccui칩n a GPU antes de ejecutar el c칩digo.**"
      ],
      "metadata": {
        "id": "la-winTcNxk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalar librer칤as y dependencias**\n",
        " \n",
        "*~2 minutos de instalaci칩n*"
      ],
      "metadata": {
        "id": "KDmfHItfEJhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v8Pv_QmIByp"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q langdetect\n",
        "!pip install -q gradio\n",
        "!pip install -q torch\n",
        "!pip install -q gtts\n",
        "import torch\n",
        "import whisper\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "from peft import PeftModel\n",
        "from langdetect import detect\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar y configurar el modelo Alpaca-lora (LLaMA) de 7B par치metros.\n",
        "\n",
        "*~3 minutos de ejecuci칩n*"
      ],
      "metadata": {
        "id": "Ta6Xc9osERga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "modelo = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "modelo = PeftModel.from_pretrained(modelo, \"tloen/alpaca-lora-7b\")"
      ],
      "metadata": {
        "id": "TuFUR-QpqHjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir *funciones* que permiten construir un **sistema de preguntas y respuestas basado en tareas**."
      ],
      "metadata": {
        "id": "nrQM7lAQRE5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_instruccion(instruccion, input=None):\n",
        "    input_section = f\"### Entrada:\\n{input}\\n\" if input else \"\"\n",
        "    return f\"\"\"A continuaci칩n se muestra una instrucci칩n que describe una tarea. Escribe una respuesta que complete adecuadamente la petici칩n.\n",
        "\n",
        "### Instrucci칩n:\n",
        "{instruccion}\n",
        "\n",
        "{input_section}### Respuesta:\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        "    num_return_sequences=1,\n",
        "    max_length=512,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = gen_instruccion(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(modelo.device)\n",
        "    generation_output = modelo.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    sequence = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(sequence)\n",
        "    response_text = output.split(\"### Respuesta:\")[-1].strip()\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "f2viuCuqqtMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Interfaz Gradio**\n",
        "\n",
        "Al hacer clic en el bot칩n de grabaci칩n, el micr칩fono comenzar치 a grabar. El usuario tiene tambi칠n la opci칩n de seleccionar el tama침o del modelo de Whisper. \n",
        "\n",
        "Cuanto m치s grande sea el modelo de Whisper mejor ser치 pero tardar치 m치s.\n",
        "\n",
        "1.   **Tiny** 39 M par치metros\n",
        "2.   **Base** 74 M par치metros **[recomendado]**\n",
        "3.   **Small** 244 M par치metros\n",
        "4.   **Medium** 269 M par치metros\n",
        "5.   **Large** 1550 M par치metros\n",
        "\n",
        "\n",
        "\n",
        "Una vez que se env칤a la grabaci칩n, se transcribir치 autom치ticamente el audio con Whisper y LLaMa responder치 con un mensaje. El mensaje estar치 en formato de texto y audio, junto con la transcripci칩n del audio original."
      ],
      "metadata": {
        "id": "JyASUjp3LTz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stt(tmp_filename, model_size):\n",
        "    try:\n",
        "    # Carga el modelo de Whisper\n",
        "        modelo = whisper.load_model(model_size)\n",
        "\n",
        "    # Transcribe el audio a texto y despu칠s genera una respuesta\n",
        "        result = modelo.transcribe(tmp_filename)\n",
        "\n",
        "        text_input = result['text']\n",
        "        response_text = evaluate(text_input)\n",
        "\n",
        "    # Genera respuesta de voz usando Google TTS y la guarda\n",
        "        idioma = detect(response_text)\n",
        "        tts = gTTs(response_text, lang=idioma)\n",
        "        _, response_audio_path = tempfile.mkstemp(suffix='.mp3')\n",
        "        tts.save(response_audio_path)\n",
        "\n",
        "        return text_input, response_audio_path, response_text\n",
        "\n",
        "    except (ValueError, whisper.exceptions.AudioProcessingError, Exception) as e:\n",
        "        error_message = str(e)\n",
        "\n",
        "        return f\"Error: {error_message}\", None, None\n",
        "\n",
        "# Interfaz de Gradio\n",
        "inputs = [\n",
        "    gr.Audio(source='microphone', type='filepath', label=\"Habla aqu칤 游딖勇끂"),\n",
        "    gr.Dropdown(choices=['tiny', 'base', 'small', 'medium', 'large'], value='base', label=\"Tama침o del modelo 游닍\"),\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Transcripci칩n de tu input 游닇\"),\n",
        "    gr.Audio(type='filepath', label=\"Respuesta 游댉\"),\n",
        "    gr.Textbox(label=\"Respuesta 游늯\"),\n",
        "]\n",
        "\n",
        "pavdemo = gr.Interface(\n",
        "    fn=stt,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"WhiTTsper the Lora\",\n",
        "    description=\"Esta demo utiliza tecnolog칤as de reconocimiento de voz y s칤ntesis de voz, Whisper de OpenAI y Google Text-to-Speech respectivamente, para interactuar con la IA Alpaca-LoRA ~ modelo LLaMa 7B ).\",\n",
        ").launch()"
      ],
      "metadata": {
        "id": "z3CjDuXsqPWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}